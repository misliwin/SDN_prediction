{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "All computational and statistics packages have been tested in Anaconda environment (python 3.6)\n",
    "1. Install Anaconda\n",
    "https://www.anaconda.com/download/#linux\n",
    "2. Install Spark and Java 8\n",
    "https://medium.com/@GalarnykMichael/install-spark-on-ubuntu-pyspark-231c45677de0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Install \"pyspark\"***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"try:\\n    print('Trying to import pyspark...')\\n    import pyspark\\nexcept ImportError:\\n    print('Pyspark import failed...')\\n    print('Installing pyspark in conda environment...')\\n    import sys\\n    !conda install --yes --prefix {sys.prefix} pyspark\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"try:\n",
    "    print('Trying to import pyspark...')\n",
    "    import pyspark\n",
    "except ImportError:\n",
    "    print('Pyspark import failed...')\n",
    "    print('Installing pyspark in conda environment...')\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} pyspark\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create Spark Context***\n",
    "If you enqounter any error, try to reconfigure Spark on your machine (link in the first cell - .bashrc file configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    conf = SparkConf().setAppName('SDN')\n",
    "    sc = SparkContext(conf=conf)\n",
    "else:\n",
    "    if sc != None:\n",
    "        sc.stop()\n",
    "    conf = SparkConf().setAppName('SDN')\n",
    "    sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SQL context for spark computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import scipy.stats as sts\n",
    "\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "from apscheduler.triggers.interval import IntervalTrigger\n",
    "import matplotlib.dates as mdates\n",
    "import json\n",
    "import urllib\n",
    "import logging, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to import paramiko...\n",
      "Paramiko imported.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Trying to import paramiko...')\n",
    "    import paramiko\n",
    "    print('Paramiko imported.')\n",
    "except ImportError:\n",
    "    print('Paramiko import failed...')\n",
    "    print('Installing paramiko in conda environment...')\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} paramiko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramiko installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCP installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to import scp...\n",
      "Scp imported.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Trying to import scp...')\n",
    "    import scp\n",
    "    print('Scp imported.')\n",
    "except ImportError:\n",
    "    print('Scp import failed...')\n",
    "    print('Installing scp in conda environment...')\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to import keras...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras imported.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Trying to import keras...')\n",
    "    import keras\n",
    "    print('Keras imported.')\n",
    "except ImportError:\n",
    "    print('Keras import failed...')\n",
    "    print('Installing keras in conda environment...')\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loginanddownload(hostname,uname,pwd,sfile,tfile):\n",
    "    \"\"\"\n",
    "    Can copy files and directories from PNDa to remote system.\n",
    "    Usage example:\n",
    "        loginanddownload(red_pnda_ip, username, password, remote_folder, local_destination)\n",
    "        loginanddownload('192.168.57.4', 'pnda', 'pnda', '/data', '/home/amadeusz/')\n",
    "    \n",
    "    I am using it only for download full copy of /data folder from pnda VM. There is a dependency of openssh-server\n",
    "    installation on red_pnda VM.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Establishing ssh connection\")\n",
    "        ssh_client = paramiko.SSHClient()\n",
    "        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh_client.connect(hostname=hostname, username=uname, password=pwd)\n",
    "    except paramiko.AuthenticationException:\n",
    "        print(\"Authentication failed, please verify your credentials: %s\")\n",
    "    except paramiko.SSHException as sshException:\n",
    "        print(\"Unable to establish SSH connection: %s\" % sshException)\n",
    "    except paramiko.BadHostKeyException as badHostKeyException:\n",
    "        print(\"Unable to verify server's host key: %s\" % badHostKeyException)\n",
    "    except Exception as e:\n",
    "        print(e.args)\n",
    "    try:\n",
    "        print(\"Getting SCP Client\")\n",
    "        scpclient = scp.SCPClient(ssh_client.get_transport())\n",
    "        print(\"Hostname: %s\", hostname)\n",
    "        print(\"source file: %s\", sfile)\n",
    "        print(\"target file: %s\", tfile)\n",
    "        scpclient.get(sfile,tfile, recursive = True)\n",
    "    except scp.SCPException as e:\n",
    "        print(\"Operation error: %s\", e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    \"\"\"\n",
    "    Prediction class - class for preprocessing data from red_pnda.\n",
    "    Not full variables are in use (this is a changed copy of Lecturer shared file)\n",
    "    \n",
    "    Prediction class takes exacly one argument - bytes (network traffic data)\n",
    "    It is further processed and returned in other format.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, bytes):\n",
    "        self.bytes = bytes\n",
    "        self.omega = 2.0 * np.pi / len(bytes)\n",
    "        self.export = pd.DataFrame()\n",
    "        self.x = None\n",
    "        self.data = None\n",
    "        self.y = None\n",
    "        self.train = None\n",
    "        self.index = None\n",
    "        self.train_mean = None\n",
    "        self.train_std = None\n",
    "\n",
    "    def prepare_data_for_prediction(self):\n",
    "        \"\"\"\n",
    "        From RAW data compute time dependency (x) and bandwidth (y).\n",
    "        \"\"\"\n",
    "        self.train = self.bytes\n",
    "        self.train_mean = np.mean(self.train)\n",
    "        self.train_std = np.std(self.train)\n",
    "        self.train = (self.train - self.train_mean) / self.train_std\n",
    "        self.index = np.asarray(range(len(self.train)), dtype=np.float64)\n",
    "        self.x = np.asarray(range(len(self.train)), dtype=np.float64)\n",
    "        self.y = self.train\n",
    "        self.data = pd.DataFrame(np.column_stack([self.x, self.y]), columns=['x', 'y'])\n",
    "\n",
    "    def proceed_prediction(self, percent):\n",
    "        \"\"\"\n",
    "        Start data preprocessing.\n",
    "        \"\"\"\n",
    "        self.prepare_data_for_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Load the data***\n",
    "Preprocessed data will be stored in wd.resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "npzfile = np.load(\"/home/misliwin/Desktop/sdn/SDN_data_preprocessing_simple_NN_with_data/test_data.npz\")\n",
    "npzfile['arr_0']\n",
    "X_test_complete = npzfile['arr_0']\n",
    "Y_test_complete = npzfile['arr_1']\n",
    "series = npzfile['arr_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network part (only one step ahead prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "    df = pd.DataFrame(data)\n",
    "    columns = [df.shift(i) for i in range(1, lag+1)]\n",
    "    columns.append(df)\n",
    "    df = pd.concat(columns, axis=1)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    " \n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return pd.Series(diff)\n",
    " \n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    return yhat + history[-interval]\n",
    " \n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "    # fit scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train)\n",
    "    # transform train\n",
    "    train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_scaled = scaler.transform(train)\n",
    "    # transform test\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = scaler.transform(test)\n",
    "    return scaler, train_scaled, test_scaled\n",
    " \n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "    new_row = [x for x in X] + [value]\n",
    "    array = np.array(new_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -1]\n",
    " \n",
    "# load dataset\n",
    "\n",
    "diff_values = difference(series, 1)\n",
    " \n",
    "# transform data to be supervised learning\n",
    "supervised = timeseries_to_supervised(diff_values, 1)\n",
    "supervised_values = supervised.values\n",
    " \n",
    "# split data into train and test-sets\n",
    "train, test = supervised_values[0:-2000], supervised_values[-2000:]\n",
    "\n",
    "# transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(train, test)\n",
    "range_size = 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create NN model in Keras***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 16)                8016      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 500)               8500      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 500)               250500    \n",
      "=================================================================\n",
      "Total params: 517,516\n",
      "Trainable params: 517,516\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('/home/misliwin/Desktop/sdn/SDN_data_preprocessing_simple_NN_with_data/SDN_model.h5')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test dataset without confidence interval\n",
    "predictions = model.predict(X_test_complete, batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensorflow function to apply dropout during prediction\n",
    "import keras.backend as K\n",
    "f = K.function([model.layers[0].input, K.learning_phase()], [model.layers[-1].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 999, 1)\n",
      "0.2002002002002002\n"
     ]
    }
   ],
   "source": [
    "# invert scaling, differencing\n",
    "pred = list()\n",
    "X_vector = X_test_complete[:, -2:-1]\n",
    "Y_vector = Y_test_complete[:, -1]\n",
    "preds = predictions[:, -1]\n",
    "for i in range(len(X_test_complete)):\n",
    "    X, y = X_vector[i], Y_vector[i]\n",
    "    #print(X)\n",
    "    # invert scaling\n",
    "    yhat = invert_scale(scaler, X, preds[i])\n",
    "\n",
    "    # invert differencing\n",
    "    yhat = inverse_difference(series, yhat, len(X_test_complete)+1-i)\n",
    "    # store forecast\n",
    "    pred.append(yhat)\n",
    "    expected = series[len(train) + i + 1]\n",
    "    \n",
    "def estimate_gaussian(X):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma2 = np.var(X, axis=0)\n",
    "    return mu, sigma2        \n",
    "pred_array = np.array(pred)\n",
    "mu, sigma2 = estimate_gaussian(pred)\n",
    "\n",
    "lower_bound = 0.2\n",
    "upper_bound = 0.8\n",
    "\n",
    "low = np.quantile(pred_array, lower_bound)\n",
    "upp = np.quantile(pred_array, upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.233\n"
     ]
    }
   ],
   "source": [
    "# performance \n",
    "rmse = np.sqrt(mean_squared_error(series[-2000+2*range_size+1:-1], pred[1:]))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_threshold(y_val, p_val):\n",
    "    step_size = (np.max(p_val) - np.min(p_val)) / 1000\n",
    "\n",
    "    best_epsilon = 0.0\n",
    "    best_F1 = 0.0\n",
    "\n",
    "    for epsilon in np.arange(min(p_val), max(p_val), step_size):\n",
    "        predictions = p_val < epsilon\n",
    "        tp = np.sum(predictions[np.nonzero(y_val == True)])\n",
    "        fp = np.sum(predictions[np.nonzero(y_val == False)])\n",
    "        fn = np.sum(y_val[np.nonzero(predictions == False)] == True)\n",
    "        if tp != 0:\n",
    "            prec = 1.0 * tp / (tp + fp)\n",
    "            rec = 1.0 * tp / (tp + fn)\n",
    "            F1 = 2.0 * prec * rec / (prec + rec)\n",
    "            if F1 > best_F1:\n",
    "                best_F1 = F1\n",
    "                best_epsilon = epsilon\n",
    "\n",
    "    return best_epsilon, best_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(pred[1:]):\n",
    "    real_data_item = series[-2000+2*range_size+1:-1][i]\n",
    "    diff = np.absolute(item - real_data_item)/item\n",
    "    if diff > 0.1 and real_data_item < low and real_data_item > upp:\n",
    "        print(\"anomaly\")\n",
    "    #print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = 0\n",
    "if plot:\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(series[-2000+2*range_size+1:-1], 'b')\n",
    "    plt.plot(pred[1:], 'g-.') # predicted mean values\n",
    "    plt.plot(low*np.ones(len(pred[1:])), 'r') # predicted mean values\n",
    "    plt.plot(upp*np.ones(len(pred[1:])), 'r') # predicted mean values\n",
    "\n",
    "#plt.plot(pred_array[1:]*1.1, 'r')\n",
    "#plt.plot(pred_array[1:]*0.1, 'r')\n",
    "#plt.fill_between(x = range(1999), y1 = p[1:]+u[1:], y2=p[1:]-u[1:]) # predicted uncertanity interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
